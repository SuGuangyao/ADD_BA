{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDS22-ptsF8E","executionInfo":{"status":"ok","timestamp":1675072749202,"user_tz":-480,"elapsed":755,"user":{"displayName":"GY Su","userId":"04173587347002726584"}},"outputId":"65722d77-2b90-4212-ee03-f1df0b27aa44"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import joblib\n","import os\n","\n","from torch.utils.data import Dataset\n","import numpy as np\n","import joblib\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","\n","\n","class Air_Dataset(Dataset):\n","\n","    def __init__(self,\n","                 input_len: int = 64,\n","                 output_len: int = 1,\n","                 train: bool = True,\n","                 transformer: bool = False,\n","                 train_test_split_ratio: float = 0.8,\n","                 file_path: str = '../data/AirQualityUCI/data_array/air.array'):\n","        super(Air_Dataset, self).__init__()\n","\n","        self.prsa_array = joblib.load(file_path)\n","\n","        self.train_array = self.prsa_array[:int(self.prsa_array.shape[0] * train_test_split_ratio)]\n","        self.test_array = self.prsa_array[int(self.prsa_array.shape[0] * train_test_split_ratio):]\n","\n","        # 注意训练集和测试集要分开做归一化，否则会造成数据泄露\n","        self.train_scaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.test_scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","        self.tranformed_train_array = self.get_transform(self.train_scaler, self.train_array)\n","        self.tranformed_test_array = self.get_transform(self.test_scaler, self.test_array)\n","\n","        self.sequence_list = list()\n","        self.target_list = list()\n","        \n","        if not transformer:\n","            if train:\n","                for i in range(self.tranformed_train_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_train_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_train_array[i+input_len: i+input_len+output_len])\n","            else:\n","                for i in range(self.tranformed_test_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_test_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_test_array[i + input_len: i + input_len + output_len])\n","        else:\n","            if train:\n","                for i in range(self.tranformed_train_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_train_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_train_array[i+output_len: i+input_len+output_len])\n","            else:\n","                for i in range(self.tranformed_test_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_test_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_test_array[i + output_len: i + input_len + output_len])\n","\n","\n","    def __getitem__(self, idx):\n","        sequence = torch.tensor(self.sequence_list[idx], dtype=torch.float32)\n","        target = torch.tensor(self.target_list[idx], dtype=torch.float32)\n","\n","        return sequence, target\n","\n","    def __len__(self):\n","        return len(self.sequence_list)\n","\n","    def get_transform(self, scaler: MinMaxScaler, array: np.array):\n","        return scaler.fit_transform(array)\n","\n","    def get_inverse_transfoerm(self, scaler: MinMaxScaler, array: np.array):\n","        return scaler.inverse_transform(array)\n","\n"],"metadata":{"id":"HUYwZwirAAgF","executionInfo":{"status":"ok","timestamp":1675072824845,"user_tz":-480,"elapsed":1103,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzT88j8D_Yat","executionInfo":{"status":"ok","timestamp":1675072754562,"user_tz":-480,"elapsed":4799,"user":{"displayName":"GY Su","userId":"04173587347002726584"}},"outputId":"e0af390b-e827-4562-9a0f-e24dfbd97351"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class AttBLSTM(nn.Module):\n","    \n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.2):\n","        super(AttBLSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.blstm = nn.LSTM(self.input_size,\n","                             self.hidden_dim,\n","                             num_layers=2,\n","                             bidirectional=True,\n","                             batch_first=True,\n","                             dropout=self.lstm_dropout)\n","\n","        self.tanh = nn.Tanh()\n","        self.w = nn.Parameter(torch.Tensor(self.hidden_dim * 2, 1))\n","        torch.nn.init.kaiming_normal_(self.w)\n","        self.sigmoid = nn.Sigmoid()\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim * 2, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.blstm(x)\n","\n","        M = self.tanh(out)\n","        alpha = F.softmax(torch.matmul(M, self.w), dim=1)\n","        out = out * alpha\n","        out = torch.sum(out, dim=1)\n","\n","        # out = self.tanh(out)\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"p3v_NC2I-_Gy","executionInfo":{"status":"ok","timestamp":1675072758988,"user_tz":-480,"elapsed":4428,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class BLSTM(nn.Module):\n","\n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.1):\n","        super(BLSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.blstm = nn.LSTM(self.input_size,\n","                             self.hidden_dim,\n","                             num_layers=2,\n","                             bidirectional=True,\n","                             batch_first=True,\n","                             dropout=self.lstm_dropout)\n","\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim * 2, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.blstm(x)\n","\n","        out = out[:, -1, :]\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"UcmgXpRd_A3F","executionInfo":{"status":"ok","timestamp":1675072758988,"user_tz":-480,"elapsed":12,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class LSTM(nn.Module):\n","\n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.1):\n","        super(LSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.lstm = nn.LSTM(self.input_size,\n","                            self.hidden_dim,\n","                            num_layers=1,\n","                            bidirectional=False,\n","                            batch_first=True,\n","                            dropout=self.lstm_dropout)\n","\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.lstm(x)\n","\n","        out = out[:, -1, :]\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"SCrmZMZF_F-9","executionInfo":{"status":"ok","timestamp":1675072758989,"user_tz":-480,"elapsed":13,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"id":"oO1OW6pz-kN-","outputId":"6ff63f5b-1397-479b-c68f-0c5dcae23fbf","executionInfo":{"status":"error","timestamp":1675073472916,"user_tz":-480,"elapsed":844,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["输入长度为16\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a3fa6f625f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mfinal_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m           \u001b[0mfinal_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-a3fa6f625f17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, learning_rate, epochs, input_len, step_lr, lr_change_step, gamma, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           device: str = 'cuda'):\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_change_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}],"source":["import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.cuda import amp\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","import os\n","import joblib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# from models import LSTM, AttBLSTM, Transformer, LinearTransformer, CNNTransformer, CNNTransformerHighway, MaskedLinearTransformer\n","# from utils import PRAS_Dataset\n","\n","'''\n","Typical Mixed Precision Training\n","'''\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    Clips gradients computed during backpropagation to avoid explosion of gradients.\n","\n","    :param optimizer: optimizer with the gradients to be clipped\n","    :param grad_clip: clip value\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group[\"params\"]:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","def train(model: nn.Module,\n","          train_loader: DataLoader,\n","          test_loader: DataLoader,\n","          learning_rate: float,\n","          epochs: int,\n","          input_len : int = 64,\n","          step_lr: bool = True,\n","          lr_change_step: int = 10,\n","          gamma: float = 0.99,\n","          device: str = 'cuda'):\n","    model.to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_change_step, gamma=gamma)\n","    loss_caculate = nn.MSELoss().to(device)\n","    loss_ca1 = nn.L1Loss().to(device)\n","    scaler = amp.GradScaler(enabled=True)\n","\n","    train_loss_list = list()\n","    test_loss_list = list()\n","    test_loss = torch.Tensor([float('inf')])\n","    ##修改路径和文件夹 模型名称\n","    weight_file_path = '/content/drive/MyDrive/ADD/air_data_weight_LSTM/'\n","\n","    if not os.path.exists(weight_file_path):\n","        os.mkdir(weight_file_path)\n","    break_flag = 0\n","    with tqdm(total=epochs, desc='Model Training') as pbar:\n","        for epoch in range(1, epochs + 1):\n","           \n","            train_total_loss = 0.\n","            test_total_loss = 0.\n","            train_total_loss1 = 0.\n","            test_total_loss1 = 0.\n","            '''\n","            Train model\n","            '''\n","            model.train()\n","            loss = None\n","\n","            for i in train_loader:\n","                sequence = i[0].to(device)\n","                target = i[1].squeeze(1).to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with amp.autocast(enabled=True):\n","                    ouputs = model(sequence)\n","                    loss = loss_caculate(ouputs, target[:, -1, :])\n","                    loss1 = loss_ca1(ouputs, target[:, -1, :])\n","                scaler.scale(loss).backward()\n","                clip_gradient(optimizer,10e5)\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","                train_total_loss += loss.item()\n","                train_total_loss1 += loss1.item()\n","                \n","\n","            if step_lr:\n","                scheduler.step()\n","\n","            '''\n","            Evaluate model\n","            '''\n","            model.eval()\n","            loss = None\n","\n","            with torch.no_grad():\n","                for i in test_loader:\n","                    sequence = i[0].to(device)\n","                    target = i[1].squeeze(1).to(device)\n","\n","                    ouputs = model(sequence)\n","                    loss = loss_caculate(ouputs, target[:, -1, :])\n","                    # loss = loss_caculate(ouputs, target)\n","                    loss1 = loss_ca1(ouputs, target[:, -1, :])\n","                    test_total_loss += loss.item()\n","                    test_total_loss1 += loss1.item()\n","                if test_total_loss <= test_loss:\n","                    for name in os.listdir(weight_file_path):\n","                        file = weight_file_path + '/' + name\n","                        # print(int(name.split('_')[-1].split('.')[0]))\n","                        try:\n","                          if int(name.split('_')[-1].split('.')[0]) == input_len:\n","                            os.remove(file)\n","                        except:\n","                          pass  \n","                    torch.save(model.state_dict(), weight_file_path + '/' + '{}_{}_{}_{}_{}.pt'.format(model.name(), epoch, test_total_loss, test_total_loss1, input_len))\n","                    test_loss = test_total_loss\n","                    final_name = weight_file_path + '/' + '{}_{}_{}_{}_{}.pt'.format(model.name(), epoch, test_total_loss, test_total_loss1, input_len)\n","                    break_flag = 0\n","                else:\n","                  break_flag += 1\n","            # train_loss_list.append(train_total_loss)\n","            # joblib.dump(train_loss_list, './logs/{}_train_loss_test_amp_0.list'.format(model.name()))\n","            # test_loss_list.append(test_total_loss)\n","            # joblib.dump(test_loss_list, './logs/{}_test_loss_test_amp_0.list'.format(model.name()))\n","\n","            tqdm.write('Epoch: {:5} | Train Loss: {:8}| Train MAE: {:8} | Test Loss: {:8}| Test MAE: {:8}  | LR: {:8}'.format(epoch, train_total_loss,train_total_loss1, \n","                                                                                          test_total_loss, test_total_loss1,\n","                                                                                            scheduler.get_last_lr()[0]))\n","\n","            pbar.update(1)\n","            if epoch>=350 and break_flag>=50:\n","              break \n","    return final_name\n","def test(model: nn.Module, \n","         test_dataset: Air_Dataset,\n","         device: str = 'cuda'):\n","    model.to(device)\n","    model.eval()\n","    loss_caculate = nn.MSELoss().to(device)\n","    loss_ca1 = nn.L1Loss().to(device)\n","    test_total_loss = 0.\n","    test_total_loss1 = 0.\n","    ##修改路径及文件夹名称\n","    weight_file_path = '/content/drive/MyDrive/ADD/air_data_weight_LSTM/'\n","    predict_array_list = list()\n","    target_array_list = list()\n","\n","    for i in test_dataset:\n","        input_tensor = i[0].unsqueeze(0).to(device)\n","        target_tensor = i[1]\n","\n","        with torch.no_grad():\n","            output_tensor = model(input_tensor).squeeze(0).cpu()\n","\n","            loss = loss_caculate(output_tensor, target_tensor[-1])\n","            test_total_loss += loss.item()\n","\n","            loss1 = loss_ca1(output_tensor, target_tensor[-1])\n","            test_total_loss1 += loss1.item()\n","\n","            predict_array_list.append(output_tensor.numpy())\n","            target_array_list.append(target_tensor[-1].numpy())\n","\n","    input_array = test_dataset[0][0].cpu().numpy()\n","    predict_array = np.array(predict_array_list)\n","    target_array = np.array(target_array_list)\n","\n","    return input_array.T, predict_array.T, target_array.T, test_total_loss, test_total_loss1, len(test_dataset)\n","\n","\n","if __name__ == '__main__':\n","    ##修改路径\n","    file_path = '/content/drive/MyDrive/ADD/data/air.array'\n","\n","    batch_size = 1024\n","    do_train = True\n","    learning_rate = 0.0001\n","    epochs = 4000\n","\n","    do_test = False\n","    input_lens = [16, 32, 48, 64, 80, 96, 112, 128]\n","    # 定义模型时需要确定是单一变量预测还是多变量预测\n","    model = LSTM(input_size=13, output_size=13)\n","    for input_len in input_lens:\n","      print(f\"输入长度为{input_len}\")\n","      train_dataset = Air_Dataset(input_len=input_len, train=True, file_path=file_path, transformer=True)\n","      test_dataset  = Air_Dataset(input_len=input_len,train=False, file_path=file_path, transformer=True)\n","\n","      train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=2)\n","      test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, drop_last=True, num_workers=2)\n","\n","      start = time.time()\n","      final_name = \"\"\n","      if do_train:\n","          final_name = train(model=model, train_loader=train_loader, test_loader=test_loader, input_len=input_len, learning_rate=learning_rate, epochs=epochs)\n","\n","      end = time.time()\n","\n","      # print(end - start)\n","      \n","      if do_test:\n","          # model = BLSTM(input_size=15, output_size=15)\n","          model.load_state_dict(torch.load(final_name))\n","          i, p, t, loss_MSE, loss_MAE, length = test(model=model, test_dataset=test_dataset)\n","\n","          # for plot_feature_idx in range(0, 15):\n","          #     plt.figure(plot_feature_idx)\n","          #     plt.plot(range(0, i.shape[1]), i[plot_feature_idx], )\n","          #     plt.plot(range(i.shape[1], i.shape[1] + p.shape[1]), p[plot_feature_idx], )\n","          #     plt.plot(range(i.shape[1], i.shape[1] + p.shape[1]), t[plot_feature_idx], )\n","          \n","          # plt.show()\n","\n","\n","          print('MSE: {}'.format(loss_MSE))\n","          print('MAE: {}'.format(loss_MAE))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"JMcblVjErCFf"}},{"cell_type":"code","source":[],"metadata":{"id":"oW67MPuRewE1","executionInfo":{"status":"aborted","timestamp":1675072758989,"user_tz":-480,"elapsed":4,"user":{"displayName":"GY Su","userId":"04173587347002726584"}}},"execution_count":null,"outputs":[]}]}